{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6vtSsfypQ1A"
   },
   "source": [
    "***DeadLine - 29.09.2024 23:59***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Примечание:\n",
    "\n",
    "Я убрал инструкцию, потому что мешает. \n",
    "\n",
    "С ноутбуком я работаю локально поэтому установка зависимостей ниже."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "packages = [\n",
    "    \"jupyter==1.0.0\",\n",
    "    \"folium==0.17.0\",\n",
    "    \"nltk==3.9.1\",\n",
    "    \"gensim==4.3.3\",\n",
    "    \"bokeh==3.6.0\",\n",
    "    \"matplotlib==3.9.2\",\n",
    "    \"seaborn==0.13.2\",\n",
    "    \"scikit-learn==1.5.2\",\n",
    "]\n",
    "\n",
    "with open(\"requirements.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(packages))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt > /dev/null"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HiOOuq2XHta"
   },
   "source": [
    "## Семинар 1: Веселье с векторными представлениями слов (2 балла)\n",
    "\n",
    "Сегодня мы будем играть с векторными представлениями слов: обучим собственные небольшие эмбеддинги, загрузим готовую модель из gensim model zoo и используем её для визуализации текстового корпуса.\n",
    "\n",
    "Вся работа будет происходить на наборе данных с векторными представлениями.\n",
    "\n",
    "__Требования:__  `pip install --upgrade nltk gensim bokeh`, но только если вы запускаете код локально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "x_XuuradXHtc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0dddf85d-f4db-4f1d-a2e7-3be55c2eb240"
   },
   "outputs": [],
   "source": [
    "!wget \"https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\" -O ./quora.txt"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "id": "UbzjAwEcXHtd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "462017ff-30f2-4b49-8469-8809cffcdc3d"
   },
   "outputs": [],
   "source": [
    "with open(\"./quora.txt\", encoding=\"utf-8\") as file:\n",
    "    data = list(file)\n",
    "\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dA1HAj5XHtd"
   },
   "source": [
    "__Токенизация:__ типичный первый шаг для задачи обработки естественного языка — это разделение исходных данных на слова. Текст, с которым мы работаем, находится в сыром формате: пунктация, смайлики и прочее\n",
    "\n",
    "Давайте воспользуемся __`nltk`__ — библиотекой, которая выполняет многие задачи NLP, такие как токенизация, стемминг или определение частей речи.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tf0mbWTiXHtd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "445358bd-93ce-49c2-cd29-2970fe7da2f2"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(data[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "JlJrfOKQXHtd"
   },
   "outputs": [],
   "source": [
    "# TASK: lowercase everything and extract tokens with tokenizer.\n",
    "# data_tok should be a list of lists of tokens for each line in data.\n",
    "\n",
    "data_tok = list(map(lambda x: tokenizer.tokenize(x.lower()), data)) # map works better \n",
    "data_tok[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "o6sBcb8xXHtd"
   },
   "outputs": [],
   "source": [
    "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "\n",
    "assert all(all(isinstance(tok, str) for tok in row) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "\n",
    "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
    "\n",
    "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yh-kZZz4XHte",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "73d89369-fea6-4771-d992-66a1878731c6"
   },
   "outputs": [],
   "source": [
    "print([' '.join(row) for row in data_tok[:2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pemtd_ReXHte"
   },
   "source": [
    "__Векторы слов:__ как известно, существует множество способов для обучения векторных представлений слов. Есть Word2Vec и GloVe с разными целевыми функциями. Затем есть fastText, который использует n-gram модели для обучения векторных представлений слов.\n",
    "\n",
    "Выбор огромен, так что давайте начнем с чего-то простого: __gensim__ – еще одна библиотека для обработки естественного языка, включающая множество моделей на основе векторов, включая word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "j4WHXzasXHte"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(data_tok,\n",
    "                 vector_size=32, # embedding vector size\n",
    "                 min_count=5,  # consider words that occured at least 5 times\n",
    "                 window=5).wv  # define context as a 5-word window around the target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0uT2GlgXHte",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ef67dd75-3115-4b99-ce5f-d0a04a796de7"
   },
   "outputs": [],
   "source": [
    "# now you can get word vectors !\n",
    "model.get_vector('anything')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCSBY4JyXHte",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "648fa851-cfb5-4f1c-d48c-489b3ba38d75"
   },
   "outputs": [],
   "source": [
    "# or query similar words directly. Go play with it!\n",
    "model.most_similar('bread')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bG7GLqQxXHte"
   },
   "source": [
    "### Использование предобученной модели\n",
    "\n",
    "Заняло некоторое время, да? Теперь представьте себе обучение полноразмерных (100~300 измерений) векторных представлений слов на гигабайтах текста: статьи из Википедии или посты в Твиттере.\n",
    "\n",
    "К счастью, сегодня вы можете получить предобученную модель векторных представлений слов всего за 2 строки кода (без смс и регистрации, обещаю)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "n1WkmZd1XHte",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "921a8107-0dac-4728-91fa-29abd1fd164a"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as gensim_api\n",
    "model = gensim_api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rekMBeewXHtf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b74cc492-0058-44bf-f2df-6d61c63d0294"
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"coder\", \"money\"], negative=[\"brain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NlyJ0rqXHtf"
   },
   "source": [
    "### Визуализация векторов слов\n",
    "\n",
    "Один из способов понять, насколько хороши наши векторы, — это их визуализировать. Проблема в том, что эти векторы находятся в пространстве с размерностью 30+ измерений, а мы, люди, привыкли к 2-3 измерениям.\n",
    "\n",
    "К счастью, мы, специалисты по машинному обучению, знаем о методах __снижения размерности__.\n",
    "\n",
    "Давайте используем их, чтобы построить график для 1000 самых частотных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4ptZSWTXHtf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7a34bd0c-3f10-4463-8236-9b388b9a59d2"
   },
   "outputs": [],
   "source": [
    "words = model.index_to_key[:1000]\n",
    "\n",
    "print(words[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-9VO87KXHtf"
   },
   "outputs": [],
   "source": [
    "# for each word, compute it's vector with model\n",
    "word_vectors = np.array(list(map(model.get_vector, words)))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_vectors"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ZaVLXXwBXHtf"
   },
   "outputs": [],
   "source": [
    "assert isinstance(word_vectors, np.ndarray)\n",
    "assert word_vectors.shape == (len(words), 100)\n",
    "assert np.isfinite(word_vectors).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-cWdXtsXHtf"
   },
   "source": [
    "#### Линейная проекция: PCA\n",
    "\n",
    "Самый простой метод линейного снижения размерности — это Principial Component Analysis.\n",
    "\n",
    "В геометрических терминах PCA пытается найти оси, вдоль которых происходит наибольшая дисперсия. Это можно назвать \"естественными\" осями.\n",
    "\n",
    "<img src=\"https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/pca_fish.png\" style=\"width:30%\">\n",
    "\n",
    "Под капотом алгоритм пытается разложить матрицу объект-признак $X$ на две меньшие матрицы: $W$ и $\\hat W$, минимизируя _среднеквадратичную ошибку_:\n",
    "\n",
    "$$\\|(X W) \\hat{W} - X\\|^2_2 \\to_{W, \\hat{W}} \\min$$\n",
    "- $X \\in \\mathbb{R}^{n \\times m}$ — матрица объектов (**централизованная**);\n",
    "- $W \\in \\mathbb{R}^{m \\times d}$ — матрица прямого преобразования;\n",
    "- $\\hat{W} \\in \\mathbb{Р}^{d \\times m}$ — матрица обратного преобразования;\n",
    "- $n$ — количество объектов, $m$ — исходная размерность, $d$ — целевая размерность.\n",
    "\n",
    "PCA работает, находя такие линейные проекции данных, которые сохраняют максимальное количество информации, измеряемое через дисперсию. Основная идея заключается в том, чтобы найти несколько новых осей (компонент), которые захватывают основную часть структуры данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "P8d--LXOXHtf"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# map word vectors onto 2d plane with PCA. Use good old sklearn api (fit, transform)\n",
    "# after that, normalize vectors to make sure they have zero mean and unit variance\n",
    "word_vectors_pca = PCA(n_components=2).fit_transform(word_vectors)\n",
    "\n",
    "word_vectors_pca = (word_vectors_pca - word_vectors_pca.mean(axis=0)) / word_vectors_pca.std(axis=0) \n",
    "\n",
    "# and maybe MORE OF YOUR CODE here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "_K5VDDXmXHtf"
   },
   "outputs": [],
   "source": [
    "assert word_vectors_pca.shape == (len(word_vectors), 2), \"there must be a 2d vector for each word\"\n",
    "assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"points must be zero-centered\"\n",
    "assert max(abs(1.0 - word_vectors_pca.std(0))) < 1e-2, \"points must have unit variance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LK7Ay8x8XHtf"
   },
   "source": [
    "#### Let's draw it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmpt4UeKXHtf"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    if isinstance(color, str): color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WaNKb59XHtg"
   },
   "outputs": [],
   "source": [
    "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)\n",
    "\n",
    "# hover a mouse over there and see if you can identify the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6ihKZm6XHtg"
   },
   "source": [
    "### Визуализация соседей с помощью t-SNE\n",
    "PCA хорош для анализа данных, но это строго линейный метод, и он способен улавливать только грубую высокоуровневую структуру данных.\n",
    "\n",
    "Если же мы хотим сосредоточиться на том, чтобы соседние точки данных оставались рядом, можно использовать t-SNE, который сам по себе является методом векторизации (embedding). Здесь можно __[узнать больше о t-SNE](https://distill.pub/2016/misread-tsne/)__.\n",
    "\n",
    "t-SNE (t-distributed Stochastic Neighbor Embedding) — это метод нелинейного снижения размерности, который лучше всего подходит для визуализации высокоразмерных данных. Он работает, моделируя вероятность того, что объекты будут соседями в исходном пространстве, и затем пытается сохранить эти вероятности в проекции меньшей размерности. Основная идея t-SNE — это сохранение локальных структур данных, что делает его полезным для кластеризации и анализа сложных структур."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbBcxzN5XHtg"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# map word vectors onto 2d plane with TSNE. hint: don't panic it may take a minute or two to fit.\n",
    "# normalize them as just lke with pca\n",
    "\n",
    "\n",
    "word_tsne = TSNE(n_components=2).fit_transform(word_vectors)\n",
    "word_tsne = (word_tsne - word_tsne.mean(axis=0)) / word_tsne.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false,
    "id": "huFK9fecXHtg"
   },
   "outputs": [],
   "source": [
    "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZ8YT9JDXHtg"
   },
   "source": [
    "### Визуализация фраз\n",
    "\n",
    "Векторные представления слов также можно использовать для представления коротких фраз. Самый простой способ — взять __среднее значение__ векторов всех токенов в фразе с определёнными весами.\n",
    "\n",
    "Этот приём полезен для понимания, с какими данными вы работаете: можно определить наличие выбросов, кластеров или других артефактов."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.has_index_for(\"hi\"), model.has_index_for(\"hidddwa\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "FnhoKn9qXHtg"
   },
   "outputs": [],
   "source": [
    "def get_phrase_embedding(phrase: str, show_debug: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert phrase to a vector by aggregating it's word embeddings. See description above.\n",
    "    \"\"\"\n",
    "    # 1. lowercase phrase\n",
    "    # 2. tokenize phrase\n",
    "    # 3. average word vectors for all words in tokenized phrase\n",
    "    # skip words that are not in model's vocabulary\n",
    "    # if all words are missing from vocabulary, return zeros\n",
    "    \n",
    "    tokens = tokenizer.tokenize(phrase.lower())\n",
    "    if show_debug:\n",
    "        print(f\"[get_phrase_embedding] Tokens before filter: {len(tokens)}\")\n",
    "    tokens_filtered = [token for token in tokens if model.has_index_for(token)]\n",
    "    if show_debug:\n",
    "        print(f\"[get_phrase_embedding] Tokens after filter: {len(tokens_filtered)}\")\n",
    "    if len(tokens_filtered) == 0:\n",
    "        return np.zeros(shape=model.vector_size)\n",
    "    \n",
    "    tokens_filtered = np.array(list(map(model.get_vector, tokens_filtered)))\n",
    "    \n",
    "    return tokens_filtered.mean(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "tpAqgAwhXHtg"
   },
   "outputs": [],
   "source": [
    "vector = get_phrase_embedding(\"I'm very sure. This never happened to me before...\")\n",
    "\n",
    "assert np.allclose(vector[::10],\n",
    "                   np.array([ 0.31807372, -0.02558171,  0.0933293 , -0.1002182 , -1.0278689 ,\n",
    "                             -0.16621883,  0.05083408,  0.17989802,  1.3701859 ,  0.08655966],\n",
    "                              dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Amy_dF75XHtg"
   },
   "outputs": [],
   "source": [
    "# let's only consider ~5k phrases for a first run.\n",
    "chosen_phrases = data[::(len(data) // 1000)]\n",
    "\n",
    "# compute vectors for chosen phrases\n",
    "phrase_vectors = np.array(list(map(get_phrase_embedding, chosen_phrases))) # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "62ELCa2bXHtg"
   },
   "outputs": [],
   "source": [
    "assert isinstance(phrase_vectors, np.ndarray) and np.isfinite(phrase_vectors).all()\n",
    "assert phrase_vectors.shape == (len(chosen_phrases), model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "lzkEPndqXHth"
   },
   "outputs": [],
   "source": [
    "# map vectors into 2d space with pca, tsne or your other method of choice\n",
    "# don't forget to normalize\n",
    "\n",
    "phrase_vectors_2d = PCA(n_components=2).fit_transform(phrase_vectors)\n",
    "\n",
    "phrase_vectors_2d = (phrase_vectors_2d - phrase_vectors_2d.mean(axis=0)) / phrase_vectors_2d.std(axis=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "xRl9JvMYXHth"
   },
   "outputs": [],
   "source": [
    "draw_vectors(phrase_vectors_2d[:, 0], phrase_vectors_2d[:, 1],\n",
    "             phrase=[phrase[:50] for phrase in chosen_phrases],\n",
    "             radius=20,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DJUDMm5XHth"
   },
   "source": [
    "Наконец, давайте создадим простой движок для поиска \"похожих вопросов\" с использованием векторных представлений фраз, которые мы создали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "_Ia1CZv4XHth"
   },
   "outputs": [],
   "source": [
    "# compute vector embedding for all lines in data\n",
    "data_vectors = np.array(list(map(lambda x: get_phrase_embedding(x, show_debug=False), data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5Di291BlXHti"
   },
   "outputs": [],
   "source": [
    "def find_nearest(query: str, k: int = 10, get_sim: bool = False):\n",
    "    \"\"\"\n",
    "    given text line (query), return k most similar lines from data, sorted from most to least similar\n",
    "    similarity should be measured as cosine between query and line embedding vectors\n",
    "    hint: it's okay to use global variables: data and data_vectors. see also: np.argpartition, np.argsort\n",
    "    \"\"\"\n",
    "    # YOUR CODE\n",
    "    query_vector: np.ndarray = get_phrase_embedding(query)\n",
    "    \n",
    "    similarities: np.ndarray = model.cosine_similarities(query_vector, vectors_all=data_vectors)\n",
    "    \n",
    "    # `nan` handling, `nan` should be shifted into out of similarity scale to left side \n",
    "    similarities[np.isnan(similarities)] = -1.\n",
    "    \n",
    "    top_k_partitioned = np.argpartition(similarities, -k)[-k:]\n",
    "    print(\"[find_nearest] top_k_partitioned.shape: \", top_k_partitioned.shape)\n",
    "    \n",
    "    top_k_indices = top_k_partitioned[np.argsort(similarities[top_k_partitioned])[::-1]]\n",
    "    print(\"[find_nearest] top_k_indices.shape: \", top_k_indices.shape)\n",
    "    print(\"=\"*100)\n",
    "    print(\"[find_nearest]: Founded top k with sim score: \", [(data[i], similarities[i]) for i in top_k_indices])\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    if get_sim:\n",
    "        top_k_results = [(data[i], similarities[i]) for i in top_k_indices]\n",
    "    else:\n",
    "        top_k_results = [data[i] for i in top_k_indices if not np.isnan(similarities[i])]\n",
    "\n",
    "    return top_k_results"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results = find_nearest(query=\"How do i enter the matrix?\", k=10)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "msaHiiVnXHti"
   },
   "outputs": [],
   "source": [
    "results = find_nearest(query=\"How do i enter the matrix?\", k=10)\n",
    "\n",
    "print(''.join(results))\n",
    "\n",
    "assert len(results) == 10 and isinstance(results[0], str)\n",
    "assert results[0] == 'How do I get to the dark web?\\n'\n",
    "assert results[3] == 'What can I do to save the world?\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "BYafEFcTXHti"
   },
   "outputs": [],
   "source": [
    "find_nearest(query=\"How does Trump?\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "yK9vCMXQXHti"
   },
   "outputs": [],
   "source": [
    "find_nearest(query=\"Why don't i ask a question myself?\", k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xka7truqcGYS"
   },
   "source": [
    "##Multilingual Embedding-based Machine Translation (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdPMOn_5cGYU"
   },
   "source": [
    "**В этой ЛР** **<font color='red'>ТЫ</font>** создашь систему машинного перевода без использования параллельных корпусов, выравнивания, внимания, сверхкрутых рекуррентных нейронных сетей с 100500-уровневой глубиной и всех прочих наворочек\n",
    "\n",
    "Но даже без параллельных корпусов эта система может быть достаточно хорошей (ну точно)\n",
    "\n",
    "Для нашей системы мы выбираем два родственных славянских языка: украинский и русский.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mtc-HbjLcGYV"
   },
   "source": [
    "### Почувствуй разницу!\n",
    "\n",
    "(_синій кіт_ vs. _синій кит_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OytE9pvHcGYV"
   },
   "source": [
    "![blue_cat_blue_whale.png](https://github.com/yandexdataschool/nlp_course/raw/master/resources/blue_cat_blue_whale.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yAhEks1cGYW"
   },
   "source": [
    "### Фрагмент списка Swadesh для некоторых славянских языков\n",
    "\n",
    "Список Swadesh - это лексико-статистический материал. Он назван в честь американского лингвиста Морриса Свадеша и содержит базовую лексику. Этот список используется для определения подгрупп языков, их родства.\n",
    "\n",
    "Таким образом, мы можем наблюдать некую инвариантность слов для разных славянских языков.\n",
    "\n",
    "\n",
    "| Russian         | Belorussian              | Ukrainian               | Polish             | Czech                         | Bulgarian            |\n",
    "|-----------------|--------------------------|-------------------------|--------------------|-------------------------------|-----------------------|\n",
    "| женщина         | жанчына, кабета, баба    | жінка                   | kobieta            | žena                          | жена                  |\n",
    "| мужчина         | мужчына                  | чоловік, мужчина        | mężczyzna          | muž                           | мъж                   |\n",
    "| человек         | чалавек                  | людина, чоловік         | człowiek           | člověk                        | човек                 |\n",
    "| ребёнок, дитя   | дзіця, дзіцёнак, немаўля | дитина, дитя            | dziecko            | dítě                          | дете                  |\n",
    "| жена            | жонка                    | дружина, жінка          | żona               | žena, manželka, choť          | съпруга, жена         |\n",
    "| муж             | муж, гаспадар            | чоловiк, муж            | mąż                | muž, manžel, choť             | съпруг, мъж           |\n",
    "| мать, мама      | маці, матка              | мати, матір, неня, мама | matka              | matka, máma, 'стар.' mateř    | майка                 |\n",
    "| отец, тятя      | бацька, тата             | батько, тато, татусь    | ojciec             | otec                          | баща, татко           |\n",
    "| много           | шмат, багата             | багато                  | wiele              | mnoho, hodně                  | много                 |\n",
    "| несколько       | некалькі, колькі         | декілька, кілька        | kilka              | několik, pár, trocha          | няколко               |\n",
    "| другой, иной    | іншы                     | інший                   | inny               | druhý, jiný                   | друг                  |\n",
    "| зверь, животное | жывёла, звер, істота     | тварина, звір           | zwierzę            | zvíře                         | животно               |\n",
    "| рыба            | рыба                     | риба                    | ryba               | ryba                          | риба                  |\n",
    "| птица           | птушка                   | птах, птиця             | ptak               | pták                          | птица                 |\n",
    "| собака, пёс     | сабака                   | собака, пес             | pies               | pes                           | куче, пес             |\n",
    "| вошь            | вош                      | воша                    | wesz               | veš                           | въшка                 |\n",
    "| змея, гад       | змяя                     | змія, гад               | wąż                | had                           | змия                  |\n",
    "| червь, червяк   | чарвяк                   | хробак, черв'як         | robak              | červ                          | червей                |\n",
    "| дерево          | дрэва                    | дерево                  | drzewo             | strom, dřevo                  | дърво                 |\n",
    "| лес             | лес                      | ліс                     | las                | les                           | гора, лес             |\n",
    "| палка           | кій, палка               | палиця                  | patyk, pręt, pałka | hůl, klacek, prut, kůl, pálka | палка, пръчка, бастун |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tXwr6VWcGYX"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WvXw6GRcGYX"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miX6d0IacGYY"
   },
   "source": [
    "\"Скачивать эмбеды надо тут:\n",
    "* [cc.uk.300.vec.zip](https://yadi.sk/d/9CAeNsJiInoyUA)\n",
    "* [cc.ru.300.vec.zip](https://yadi.sk/d/3yG0-M4M8fypeQ)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !unzip \"cc.ru.300.vec.zip\" > /dev/null"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !unzip \"cc.uk.300.vec.zip\" > /dev/null"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!ls"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaAjIJALcGYY"
   },
   "source": [
    "Качаем эмбеды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "vytZQBbzcGYY"
   },
   "outputs": [],
   "source": [
    "uk_emb = KeyedVectors.load_word2vec_format(\"cc.uk.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOTSWwPJcGYZ"
   },
   "outputs": [],
   "source": [
    "ru_emb = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwdCeP26cGYZ"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([ru_emb[\"август\"]], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DBtMhQPcGYa"
   },
   "outputs": [],
   "source": [
    "uk_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnQ8_qFgcGYa"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0ux1s2bcGYa"
   },
   "source": [
    "Построить словари для тренировочной и тестовой выгрузки (да, да, будет)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "А вот и файлы!\n",
    "\n",
    "[ukr-rus-test](https://disk.yandex.ru/i/fXtdDXKwgJloCw)\n",
    "\n",
    "[ukr-rus-train](https://disk.yandex.ru/d/lUy0sQZ1XBCchQ)"
   ],
   "metadata": {
    "id": "UF7l87PBn4Tl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bED8-0rmcGYa"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def load_word_pairs(filename: str) -> Tuple[str, np.ndarray, np.ndarray]:\n",
    "    uk_ru_pairs = []\n",
    "    uk_vectors = []\n",
    "    ru_vectors = []\n",
    "    with open(filename, \"r\") as inpf:\n",
    "        for line in inpf:\n",
    "            uk, ru = line.rstrip().split(\"\\t\")\n",
    "            if (uk not in uk_emb) or (ru not in ru_emb):\n",
    "                continue\n",
    "            uk_ru_pairs.append((uk, ru))\n",
    "            uk_vectors.append(uk_emb[uk])\n",
    "            ru_vectors.append(ru_emb[ru])\n",
    "    return uk_ru_pairs, np.array(uk_vectors), np.array(ru_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBdzXY3acGYb"
   },
   "outputs": [],
   "source": [
    "uk_ru_train, X_train, Y_train = load_word_pairs(\"ukr_rus.train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0iEld18cGYb"
   },
   "outputs": [],
   "source": [
    "uk_ru_test, X_test, Y_test = load_word_pairs(\"ukr_rus.test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaz7GgM_cGYb"
   },
   "source": [
    "## Embedding space mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsfpfHAZcGYc"
   },
   "source": [
    "Мы рассматриваем задачу обучения линейного преобразования, которое минимизирует различие между векторными представлениями слов в исходном и целевом языках.\n",
    "\n",
    "Пусть $x_i \\in \\mathbb{R}^d$ — векторное представление слова на исходном языке, а $y_i \\in \\mathbb{R}^d$ — вектор его перевода на целевой язык. Наша цель — найти матрицу $W$, которая минимизирует евклидово расстояние между преобразованными векторами $Wx_i$ и их соответствующими $y_i$.\n",
    "\n",
    "Очень похоже на линейную регрессию, не так ли?\n",
    "\n",
    "$$W^* = \\arg\\min_W \\sum_{i=1}^n ||Wx_i - y_i||_2^2,$$\n",
    "\n",
    "\n",
    "В мифологии Прокруст был известен тем, что «подгонял» людей под размеры своей кровати, растягивая их или отрезая им ноги, чтобы они идеально подходили. Подобным образом, в данной задаче мы «подгоняем» пространство встраивания исходного языка к пространству целевого языка, пытаясь свести их различия к минимуму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdwsnvbYcGYc"
   },
   "source": [
    "![embedding_mapping.png](https://github.com/yandexdataschool/nlp_course/raw/master/resources/embedding_mapping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48AY0eiUcGYd"
   },
   "source": [
    "![procrustes.png](https://github.com/yandexdataschool/nlp_course/raw/master/resources/procrustes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Целевой язык - русский, исходный - украинский"
   ],
   "metadata": {
    "id": "vABqzkMwhDKN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2j4iFsUkcGYd"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# mapping = Ridge(alpha=1.)\n",
    "mapping = LinearRegression()\n",
    "mapping.fit(X_train, Y_train)\n",
    "W = mapping.coef_"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate_model(W, X_test, Y_test):\n",
    "    transformed_vectors = X_test.dot(W.T)  # Преобразование векторов с помощью матрицы W\n",
    "    error = np.linalg.norm(transformed_vectors - Y_test, axis=1).mean()  # Средняя ошибка\n",
    "    return error\n",
    "\n",
    "# Оценка модели\n",
    "error = evaluate_model(W, X_test, Y_test)\n",
    "print(f\"Средняя ошибка на тестовых данных: {error}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFBmWRj-cGYd"
   },
   "source": [
    "Давайте посмотрим на соседей вектора слова _«серпень»_ (_«август»_ по-русски) после линейного преобразования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvKIRpuecGYd"
   },
   "outputs": [],
   "source": [
    "august = mapping.predict(uk_emb[\"серпень\"].reshape(1, -1))\n",
    "ru_emb.most_similar(august)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QyHaDoEcGYe"
   },
   "source": [
    "Верный месяц всего на 9ом месте, хм...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPoaAaREcGYe"
   },
   "source": [
    "В качестве меры качества мы будем использовать точность top@1, top@5 и top@10 (для каждого преобразованного украинского эмбеддинга мы подсчитываем, сколько правильных пар найдено в N ближайших соседях в пространстве русских эмбеддингов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkUUutZBcGYe"
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def precision(pairs, mapped_vectors, topn=1):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        pairs = список пар правильных слов [(uk_word_0, ru_word_0), ...].\n",
    "        mapped_vectors = список эмбедов после отображения из исходного пространства эмбедов в целевое пространство эмбедов\n",
    "        topn = количество ближайших соседей в целевом пространстве эмбедов, из которых нужно выбрать\n",
    "    \"\"\"\n",
    "    assert len(pairs) == len(mapped_vectors)\n",
    "\n",
    "    def check_match(i):\n",
    "        _, ru = pairs[i]\n",
    "        most_similar = ru_emb.most_similar(mapped_vectors[i], topn=topn)\n",
    "        return 1 if ru in [word for word, _ in most_similar] else 0 # Это быстрее чем цикл for и внутри смотреть if\n",
    "\n",
    "    num_matches = sum(Parallel(n_jobs=-1)(delayed(check_match)(i) for i in range(len(pairs))))\n",
    "    precision_val = num_matches / len(pairs)\n",
    "    return precision_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnDdAwKLcGYe"
   },
   "outputs": [],
   "source": [
    "assert precision([(\"серпень\", \"август\")], august, topn=5) == 0.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=9) == 1.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=10) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5Ij5230cGYe"
   },
   "outputs": [],
   "source": [
    "assert precision(uk_ru_test, X_test) == 0.0\n",
    "assert precision(uk_ru_test, Y_test) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkhDwab0cGYe"
   },
   "outputs": [],
   "source": [
    "precision_top1 = precision(uk_ru_test, mapping.predict(X_test), 1)\n",
    "precision_top5 = precision(uk_ru_test, mapping.predict(X_test), 5)\n",
    "\n",
    "assert precision_top1 >= 0.635\n",
    "assert precision_top5 >= 0.813"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "precision_top1"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "precision_top5"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Pa8k6hlcGYe"
   },
   "source": [
    "## Making it better (orthogonal Procrustean problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1-dU62jcGYe"
   },
   "source": [
    "Можно показать (см. оригинальную статью), что между семантическими пространствами должно быть ортогональным.\n",
    "\n",
    "Мы можем ограничить преобразование $W$, чтобы оно было ортогональным. Тогда мы решим следующую задачу:\n",
    "\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F \\text{, где: } W^TW = I$$\n",
    "\n",
    "$$I \\text{- единичная матрица}$$.\n",
    "\n",
    "Но ограничивать лосс в линейной регрессии как-то в падлу, поэтому мы можем найти оптимальное ортогональное преобразование с помощью разложения по сингулярным значениям (в чате тг я скидывал ссылки на краткое объяснение SVD). Оказывается, оптимальное преобразование $W^*$ может быть выражено через компоненты SVD:\n",
    "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$.\n",
    "$$W^*=UV^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDGSM-6CcGYf"
   },
   "outputs": [],
   "source": [
    "def learn_transform(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    :returns: W* : float matrix[emb_dim x emb_dim] as defined in formulae above\n",
    "    \"\"\"\n",
    "\n",
    "    M = X_train.T @ Y_train\n",
    "    U, _, Vt = np.linalg.svd(M)\n",
    "    W_star = U @ Vt\n",
    "    return W_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyOrzNhEcGYf"
   },
   "outputs": [],
   "source": [
    "W = learn_transform(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8L0rA2PcGYf"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([np.matmul(uk_emb[\"серпень\"], W)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0_Jk5FncGYf"
   },
   "outputs": [],
   "source": [
    "svd_precision = precision(uk_ru_test, np.matmul(X_test, W))\n",
    "svd_precision_top_5 = precision(uk_ru_test, np.matmul(X_test, W), 5)\n",
    "\n",
    "assert svd_precision >= 0.653\n",
    "assert svd_precision_top_5 >= 0.824"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "svd_precision"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "svd_precision_top_5"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY145_swcGYf"
   },
   "source": [
    "## UK-RU Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6prgD_icGYf"
   },
   "source": [
    "Теперь мы готовы сделать простой переводчик на основе слов: для каждого слова исходного языка в общем пространстве вкраплений мы находим ближайшее в языке перевода.\n",
    "\n",
    "[fairy_tale.txt](https://disk.yandex.ru/d/4P9xWLfm-GnauQ) - качаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DByPnd_XcGYf"
   },
   "outputs": [],
   "source": [
    "with open(\"fairy_tale.txt\", \"r\") as inpf:\n",
    "    uk_sentences = [line.rstrip().lower() for line in inpf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-eFL0ficGYg"
   },
   "outputs": [],
   "source": [
    "def translate(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        sentence - предложение на украинском языке (str)\n",
    "    :returns:\n",
    "        перевод - предложение на русском языке (str)\n",
    "\n",
    "    * найти украинские эмбеды для каждого слова в предложении\n",
    "    * преобразовать вектор украинских эмбедов\n",
    "    * найти ближайшее русское слово и заменить на него\n",
    "    \"\"\"\n",
    "    translated_words = []\n",
    "    for word in sentence.split():\n",
    "        if word in uk_emb:\n",
    "            uk_vector = uk_emb[word].reshape(1, -1)\n",
    "            ru_vector = uk_vector @ W\n",
    "            most_similar = ru_emb.most_similar(ru_vector, topn=1)\n",
    "            translated_words.append(most_similar[0][0])\n",
    "        else:\n",
    "            translated_words.append(word)  # Сохраняем слово, если его нет в эмбеддингах\n",
    "    return \" \".join(translated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knGenlVhcGYg"
   },
   "outputs": [],
   "source": [
    "assert translate(\".\") == \".\"\n",
    "assert translate(\"1 , 3\") == \"1 , 3\"\n",
    "assert translate(\"кіт зловив мишу\") == \"кот поймал мышку\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGk29vLkcGYg"
   },
   "outputs": [],
   "source": [
    "for sentence in uk_sentences:\n",
    "    print(\"src: {}\\ndst: {}\\n\".format(sentence, translate(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtX6_1PDcGYg"
   },
   "source": [
    "Очень неплохо, но как можно еще улучшить результат?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APowVdUgcGYg"
   },
   "source": [
    "## А теперь самое интересное?\n",
    "\n",
    "### Почитать:\n",
    "* [Exploiting Similarities among Languages for Machine Translation](https://arxiv.org/pdf/1309.4168)  - entry point for multilingual embedding studies by Tomas Mikolov (the author of W2V)\n",
    "* [Offline bilingual word vectors, orthogonal transformations and the inverted softmax](https://arxiv.org/pdf/1702.03859) - orthogonal transform for unsupervised MT\n",
    "* [Word Translation Without Parallel Data](https://arxiv.org/pdf/1710.04087)\n",
    "* [Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion](https://arxiv.org/pdf/1804.07745)\n",
    "* [Unsupervised Alignment of Embeddings with Wasserstein Procrustes](https://arxiv.org/pdf/1805.11222)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "4_1ac18pZYlZ"
   ],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
